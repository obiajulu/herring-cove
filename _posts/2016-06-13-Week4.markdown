---
title: Journal for Week 4
categories: journal entry
---

###Up-to-date plan for the week
1. (Monday) Main goal: Tidying up IVPTestSuite.jl
  + ~~Work towards getting `plei.jl` example fully functional, and compare performance of `ode_ab_adaptive` against RK methods (PLEI is a test case where function evaluations are expensive, which is where the AB methods are preferred. See Hairer pg 427)~~
  + ~~Skype about performance results and seek advice as to where the issues may be~~
  + ~~Have all IVPTestSuite feature branches ready for merging. First merge `m3/meta-gone`, then `ob/plei` and `ob/pyplot.`~~
2. (Tuesday) Main goal: Merge IVPTestSuite feature branches
  + ~~Finish getting `plei.jl` functional~~
  + ~~Merge the IVPTestSuite branch ob/plei into master~~
  + ~~Continue working towards merging ob/pyplot into master~~
3. (Wednesday) Main goal: Collect data on performance and continue debugging
  + ~~Run comparison plots for different solvers against `ode_ab_adaptive`~~
  + ~~Experiment with fixed step AB and Runge-Kutta methods, to compare performance when the same `tspan` is used~~
4. (Thursday) Main goal: Start preparing for JuliCon; continue to improve `ode_ab_adaptive`'s performance
  + Continue experimenting with the performance of `ode_ab_adaptive`, `ode_ab_fixed` against fixed and adaptive Runge Kutta solvers on various test cases
  + ~~Look more into BenchmarkTools.jl for today's Skype call;~~
  +~~   Skype call with Mauro about JuliaCon statergy~~
    - What to do about BenchmarkTools.jl
    - Discuss my proposed presentation approach
    - Discuss if there are other ODE.jl members I should meet with (and maybe discuss pwl's PR with)
    - Discuss if there are any developers of other ODE solvers whom I should talk, especially those who developed solvers we may include in IVPTestSuite. 

5. (Friday) Main goal: Prepare for JuliaCon; Look into BenchmarkTools.jl and wrapping other solvers;  start preparing demonstration and documentations for new solver and IVPTestSuite workflow
  + ~~Start working on Julia Notebook of new solver on PLEI testcase~~
  + ~~ Start document performance test "workflow" so that users can easily test solvers~~
  + ~~Continue preparing JuliaCon presentation~~

6. (Saturday-Sunday)Main goal: Prepare for JuliaCon; Tidy up `ode_ab_adaptive` and IVPTestSuite.jl
  + Continue to work on improving performance of `ode_ab_adaptive` until it is comparable to `ode45` an d`ode78`
  + ~~Focus on stylistic changes to make sure code is consistent with rest of ODE.jl release~~

Journal Entry #16
+ Today I learned even more about git, and did several rebasings and squashings. Now, IVPTestSuite has metaprogramming gone, and there is a pull request to include the PLEI testcase, which is just one commit. Further the ob/pyplot2 branch which is based on ob/plei, which is just one commit ahead of ob/plei. 
+ Unfornately, I wasn't able to get to the bottom of what if causing the PLEI testcase to break. I plan on running some plots tomorrow, to make sure that it is not the case the the reference solution is incorrect. Though, there is likely a small bug somewhere. In fact, I found on small bug, though there is likely another somewhere.
+ Overall, after the PLEI test case is working, we can merge with master, than subsequently, merge ob/pyplot2, in which case there will be just the master branch of IVPTestSuite. I plan to have this all done by lunch tomorrow. Then, the singular focus for the rest of the week is making the VSVO AB solver correct. 

Journal Entry #17
+ The focus for today was IVPTestSuite, and merging the feature branches. We first merged m3/meta-gone into master, and then ob/plei. After rebasing ob/pyplot2, I openned a pull request to merge this into master as well. There is still some residual references to Winston for plotting, so I have to clean these up before we can complete the merge.
+ I continued to examine the performance of `ode_ab_adaptive` using IVPTestSuite, and am seeing poor results. I'm going to try to get to the bottom of the poor performance this week.

Journal Entry #18
+ I ran a battery of tests examining the performance of various solvers at various time spans. For example, I am curious to see how ode78 performs when force feed ode_ab_adaptive's step choices, and vice verse. For example, I see that
ode45 has the same performance as ode_ab_adaptive when it uses the steps which ode_ab_adaptive outputs. I'm starting to think that the performance maybe what is expected. I included two new solvers, for testing's purposes, in ob/a-b_adaptive: ode_ab_fixed and ode78f which are fixed stepped versions of ode_ab_adaptive and ode78. Running [the following script](https://gist.github.com/obiajulu/bf51e70283d3c343fe3c576680aac9c9), I get the following results:
  + Running ode78, then ode_ab_fixed and ode4ms with same tspan:
    + ode78:```0.048609 seconds (365.18 k allocations: 11.472 MB)
abs((pos[end] - ic) ./ [ic[1],1,1,ic[4]]) = [2.3496853914830866e-11,6.293435148290328e-11,1.0298076122752797e-8,1.8166776207396305e-9]```
      + ode_ab_fixed: ```  0.030161 seconds (223.50 k allocations: 10.701 MB)
abs((pos[end] - ic) ./ [ic[1],1,1,ic[4]]) = [0.0008514817839514464,0.010158178143821374,0.6887035831286313,0.36802746888707516]```
      + ode4ms:  ```0.006645 seconds (51.60 k allocations: 3.031 MB)
abs((pos[end] - ic) ./ [ic[1],1,1,ic[4]]) = [0.0007672190085486067,0.00997421368469363,0.6910536150658455,0.36233092581519205]```

  + Running ode_ab_adaptive, then ode78_fixed and ode4ms with same tspan
    + ode_ab_adaptive:```0.275656 seconds (2.62 M allocations: 74.783 MB, 5.93% gc time)
abs((pos[end] - ic) ./ [ic[1],1,1,ic[4]]) = [0.0005981619640005499,0.0028106794731996843,0.3641672470209408,0.09662545830449291]```
      + ode78_fixed: ```  0.009091 seconds (69.59 k allocations: 2.149 MB)
abs((pos[end] - ic) ./ [ic[1],1,1,ic[4]]) = [6.167607181277784e-5,0.00014747394203141074,0.023917067023683625,0.00493469740030375]```
      + ode4ms ```0.007884 seconds (83.64 k allocations: 4.913 MB)
abs((pos[end] - ic) ./ [ic[1],1,1,ic[4]]) = [0.0005054293139100048,0.002315323571133438,0.3141324004722242,0.07558923412335078]```

+ I doubled checked to make sure that the two sets of 3 trial runs all use the same
tspan. Notice that the ode_ab_* has the same performance results as ode4ms when they use identicial tspan which is to be expected, since they are the same method. However, ode78 performs better than both of these when it can pick is tspan adaptive, but even when it is force feed the tspan which ode_ab_adaptive choose. And this is not paritcular to only ode_ab_adaptive but also ode4ms!

Journal Entry #19 (Thursday, Friday, and Weekend)
+ The last few days of the week, I put intense focus on getting `ode_ab_adaptive` to be competitive with the other nonstiff adaptive solvers, like `ode78` and `ode45`. I am happy to say that this goal was met, as the following plots show. There were many small tweaks which led to the increase in performance. First, I was much more careful about how I was handling my initial step size selection scheme, for the first few steps. @mauro3 pointed out that I was using `hinit` with the incorrect order, which introduced an error at the first step which proprogated throughout the computation. The other tweak I found I had to make was to set `minstep` to an approriate size. I was originally working with `minstep = (tspan[end]-tspan[1])/1e-15` when I should have instead used a smaller `minstep`, as does `ode78`. With these changes, `ode_ab_adaptive` is performing quite well. 
+ I threw together a JuliaBox Notebook of the new PLEI test case, which I hope to add to ODE.jl
+ I ran a few plots of fixed step size solvers, graphing walltime vs scd and stepsize vs scd, in order to see if I saw the approriate association between slopes and orders. These were observed mostly in the walltime vs scd plots, and somewhat in the stepsize vs scd plots
+ I started working on my presentation for JuliaCon, and am hoping to finish the first draft by Monday. I am targetting it to an audience which does not know much about the packages used to solve ODEs in Julia, nor the development workflow of doing this. 
